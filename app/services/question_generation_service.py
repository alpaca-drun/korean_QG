from typing import List, Optional
from threading import Lock
from app.schemas.question_generation import (
    QuestionGeneration,
    QuestionGenerationRequest,
    Question,
    QuestionGenerationSuccessResponse,
    QuestionGenerationErrorResponse,
    ErrorDetail,
    GenerationMetadata,
    BatchInfo
)
from app.clients.factory import LLMClientFactory
from app.clients.base import LLMClientBase
from app.prompts.templates import PromptTemplate
from app.db.storage import save_questions_batch_to_db
from app.utils.file_path import resolve_file_paths, ensure_storage_directory
from app.core.config import settings


class QuestionGenerationService:
    """ë¬¸í•­ ìƒì„± ì„œë¹„ìŠ¤"""
    
    def __init__(self, llm_client: Optional[LLMClientBase] = None):
        """
        Args:
            llm_client: LLM í´ë¼ì´ì–¸íŠ¸ (Noneì´ë©´ ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©)
        """
        self.llm_client = llm_client or LLMClientFactory.create_client()
        
    async def generate_questions_batch(
        self,
        requests: List[QuestionGeneration],
        current_user_id: str,
        provider: Optional[str] = None
    ) -> List[QuestionGenerationSuccessResponse | QuestionGenerationErrorResponse]:
        """
        ë°°ì¹˜ ë¬¸í•­ ìƒì„±
        
        ì—¬ëŸ¬ ë¬¸í•­ ìƒì„± ìš”ì²­ì„ í•œ ë²ˆì— ì²˜ë¦¬í•©ë‹ˆë‹¤.
        - ìµœëŒ€ 10ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ìš”ì²­ì„ ë™ì‹œì— ì²˜ë¦¬ ê°€ëŠ¥
        - ê° ìš”ì²­ì€ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤
        - ê° ìš”ì²­ ë‚´ì—ì„œ 10ë¬¸í•­ì”© ë°°ì¹˜ë¡œ ë‚˜ë‰˜ì–´ ì²˜ë¦¬ë©ë‹ˆë‹¤ (ì˜ˆ: 30ë¬¸í•­ ìš”ì²­ â†’ 3ê°œ ë°°ì¹˜)
        - ê° ë°°ì¹˜ë§ˆë‹¤ ë‹¤ë¥¸ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ë©ë‹ˆë‹¤
        
        Args:
            requests: ë¬¸í•­ ìƒì„± ìš”ì²­ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 10ê°œ)
            provider: LLM ì œê³µì (ì„ íƒì‚¬í•­)
            
        Returns:
            ì„±ê³µ ë˜ëŠ” ì‹¤íŒ¨ ì‘ë‹µ ë¦¬ìŠ¤íŠ¸ (ìš”ì²­ ìˆœì„œëŒ€ë¡œ ë°˜í™˜)
        """
        # LLM í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        if provider:
            self.llm_client = LLMClientFactory.create_client(provider=provider)
        
        # ê° ìš”ì²­ì„ 10ë¬¸í•­ì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
        # ì˜ˆ: 30ë¬¸í•­ ìš”ì²­ â†’ 3ê°œ ë°°ì¹˜ (ê° 10ë¬¸í•­ì”©)
        system_prompts = []
        user_prompts = []
        counts = []
        file_paths_list = []
        file_display_names_list = []
        request_mapping = []  # ë°°ì¹˜ ê²°ê³¼ë¥¼ ì›ë˜ ìš”ì²­ì— ë§¤í•‘í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        
        # ê° ìš”ì²­ë§ˆë‹¤ school_levelì— ë”°ë¼ ë””ë ‰í† ë¦¬ ìƒì„± ë° ê²½ë¡œ ë³€í™˜
        for req_idx, req in enumerate(requests):
            total_count = req.generation_count
            batch_size = 10
            num_batches = (total_count + batch_size - 1) // batch_size  # ì˜¬ë¦¼ ê³„ì‚°
            
            # school_level ì¶”ì¶œ
            school_level = req.school_level if hasattr(req, 'school_level') else None

            print(f"ğŸŸ£ğŸŸ£ ìš”ì²­ {req_idx}: {total_count}ê°œ ë¬¸í•­ â†’ {num_batches}ê°œ ë°°ì¹˜")
            print(f"ğŸŸ£ğŸŸ£ School Level: {school_level}")

            
            # íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„± (school_levelì— ë”°ë¼ ê²½ë¡œ ê²°ì •)
            if school_level:
                ensure_storage_directory(school_level)
            
            # íŒŒì¼ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ë³€í™˜ (school_levelì— ë”°ë¼ ê²½ë¡œ ê²°ì •)
            resolved_file_paths = resolve_file_paths(
                req.file_paths, 
                school_level=school_level
            ) if req.file_paths else None
            
            # ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œ system_prompt ìƒì„± (ëª¨ë“  ë°°ì¹˜ì—ì„œ ë™ì¼)
            first_batch_request = req.model_copy()
            first_batch_request.generation_count = batch_size
            sys_prompt, _ = PromptTemplate.build_prompt(first_batch_request)
            
            # ê° ë°°ì¹˜ë§ˆë‹¤ user_prompt ìƒì„± (ë¬¸í•­ ìˆ˜ë§Œ ë‹¤ë¦„)
            for batch_idx in range(num_batches):
                batch_request = req.model_copy()  # ìš”ì²­ ë³µì‚¬
                
                # í˜„ì¬ ë°°ì¹˜ì—ì„œ ìš”ì²­í•  ë¬¸í•­ ìˆ˜ ê³„ì‚° (ë§ˆì§€ë§‰ ë°°ì¹˜ëŠ” ë‚¨ì€ ê°œìˆ˜ë§Œí¼)
                remaining = total_count - (batch_idx * batch_size)
                current_batch_size = min(batch_size, remaining)
                batch_request.generation_count = current_batch_size
                
                print(f"  ğŸ“¦ ë°°ì¹˜ {batch_idx + 1}/{num_batches}: {current_batch_size}ê°œ ë¬¸í•­")
                
                # user_promptë§Œ ë‹¤ì‹œ ìƒì„± (ë¬¸í•­ ê°œìˆ˜ê°€ ë°˜ì˜ë¨)
                _, user_prompt = PromptTemplate.build_prompt(batch_request)
                
                system_prompts.append(sys_prompt)  # ë™ì¼í•œ system_prompt ì¬ì‚¬ìš©
                user_prompts.append(user_prompt)
                counts.append(current_batch_size)  # ì‹¤ì œ ë°°ì¹˜ í¬ê¸°
                file_paths_list.append(resolved_file_paths)  # ë³€í™˜ëœ ê²½ë¡œ ì‚¬ìš©
                file_display_names_list.append(req.file_display_names)
                
                # ì›ë˜ ìš”ì²­ ì¸ë±ìŠ¤ì™€ ë°°ì¹˜ ì •ë³´ ì €ì¥
                request_mapping.append({
                    'request_idx': req_idx,
                    'batch_idx': batch_idx,
                    'total_count': total_count,
                    'current_batch_size': current_batch_size
                })
        
        try:
            # ë°°ì¹˜ API í˜¸ì¶œ
            batch_results = await self.llm_client.generate_questions_batch(
                system_prompts=system_prompts,
                user_prompts=user_prompts,
                counts=counts,
                file_paths_list=file_paths_list,
                file_display_names_list=file_display_names_list
            )
            
            # ë°°ì¹˜ ê²°ê³¼ë¥¼ ì›ë˜ ìš”ì²­ë³„ë¡œ ê·¸ë£¹í™” (ë°°ì¹˜ ì •ë³´ í¬í•¨)
            request_questions = {}  # {request_idx: [questions]}
            request_batch_info = {}  # {request_idx: [batch_info]}
            
            for batch_result, mapping in zip(batch_results, request_mapping):
                req_idx = mapping['request_idx']
                batch_idx = mapping['batch_idx']
                
                if req_idx not in request_questions:
                    request_questions[req_idx] = []
                    request_batch_info[req_idx] = []
                
                # batch_resultëŠ” ì´ì œ Dict í˜•íƒœ: {'questions': [...], 'metadata': {...}}
                questions = batch_result.get('questions', []) if isinstance(batch_result, dict) else batch_result
                metadata = batch_result.get('metadata', {}) if isinstance(batch_result, dict) else {}
                
                print(f"ğŸ” [ë°°ì¹˜ ê²°ê³¼] req_idx={req_idx}, batch_idx={batch_idx}, ë¬¸í•­ìˆ˜={len(questions)}")
                print(f"ğŸ“Š [ë©”íƒ€ë°ì´í„°] {metadata}")
                
                if questions:
                    # ê° ë¬¸í•­ì— ë°°ì¹˜ ì •ë³´ ì¶”ê°€
                    for question in questions:
                        # ë¬¸í•­ ë°ì´í„°ì— ë°°ì¹˜ ì •ë³´ ì¶”ê°€ (dictë¡œ ë³€í™˜ í›„ ì¶”ê°€)
                        question_dict = question.model_dump() if hasattr(question, 'model_dump') else question.dict()
                        question_dict['batch_index'] = batch_idx + 1  # 1ë¶€í„° ì‹œì‘
                        request_questions[req_idx].append(question_dict)
                    
                    # ë°°ì¹˜ë³„ ì •ë³´ ê¸°ë¡ (í† í° ì •ë³´ ë° ì†Œìš” ì‹œê°„ í¬í•¨)
                    batch_info = {
                        'batch_number': batch_idx + 1,
                        'requested_count': mapping['current_batch_size'],
                        'generated_count': len(questions),
                        'input_tokens': metadata.get('input_tokens', 0),
                        'output_tokens': metadata.get('output_tokens', 0),
                        'total_tokens': metadata.get('total_tokens', 0),
                        'duration_seconds': metadata.get('duration_seconds', 0)
                    }
                    
                    # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                    if 'error' in metadata:
                        batch_info['error'] = metadata['error']
                    
                    print(f"âœ… [ë°°ì¹˜ ì •ë³´ ì €ì¥] {batch_info}")
                    request_batch_info[req_idx].append(batch_info)
            
            # ì‘ë‹µ ìƒì„± ë° DB ì €ì¥
            responses = []
            lock = Lock()
            
            for req_idx, request in enumerate(requests):
                questions = request_questions.get(req_idx, [])
                
                # ë¶€ì¡±í•œ ê²½ìš° ì¬ìš”ì²­ ë¡œì§ (ìµœëŒ€ 3íšŒ)
                retry_count = 0
                max_retries = 3
                
                while len(questions) < request.generation_count and retry_count < max_retries:
                    shortage = request.generation_count - len(questions)
                    retry_count += 1
                    
                    print(f"ğŸ”„ ì¬ìš”ì²­ {retry_count}/{max_retries}: {shortage}ê°œ ë¶€ì¡± (ìš”ì²­ {request.generation_count}ê°œ â†’ ìƒì„± {len(questions)}ê°œ)")
                    
                    try:
                        # ì¬ìš”ì²­ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
                        retry_request = request.model_copy()
                        retry_request.generation_count = shortage
                        sys_prompt, user_prompt = PromptTemplate.build_prompt(retry_request)
                        
                        # íŒŒì¼ ê²½ë¡œ í•´ê²°
                        school_level = request.school_level if hasattr(request, 'school_level') else None
                        resolved_file_paths = resolve_file_paths(
                            request.file_paths,
                            school_level=school_level
                        ) if request.file_paths else None
                        
                        # ë¶€ì¡±í•œ ë§Œí¼ë§Œ ì¬ìš”ì²­ (ë‹¨ì¼ ìš”ì²­, ë©”íƒ€ë°ì´í„° í¬í•¨)
                        retry_result = await self.llm_client.generate_questions(
                            system_prompt=sys_prompt,
                            user_prompt=user_prompt,
                            count=shortage,
                            file_paths=resolved_file_paths,
                            file_display_names=request.file_display_names,
                            return_metadata=True  # ë©”íƒ€ë°ì´í„° í¬í•¨ ìš”ì²­
                        )
                        
                        # retry_resultëŠ” Dict í˜•íƒœ: {'questions': [...], 'metadata': {...}}
                        retry_questions = retry_result.get('questions', []) if isinstance(retry_result, dict) else retry_result
                        retry_metadata = retry_result.get('metadata', {}) if isinstance(retry_result, dict) else {}
                        
                        print(f"ğŸ” [ì¬ìš”ì²­ ê²°ê³¼] ë¬¸í•­ìˆ˜={len(retry_questions)}, ë©”íƒ€ë°ì´í„°={retry_metadata}")
                        
                        if retry_questions:
                            # ì¬ìš”ì²­ ê²°ê³¼ë¥¼ dictë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€
                            for question in retry_questions:
                                question_dict = question.model_dump() if hasattr(question, 'model_dump') else question.dict()
                                question_dict['batch_index'] = f"retry_{retry_count}"  # ì¬ìš”ì²­ í‘œì‹œ
                                questions.append(question_dict)
                            
                            # ë°°ì¹˜ ì •ë³´ ì—…ë°ì´íŠ¸ (í† í° ì •ë³´ í¬í•¨)
                            if req_idx not in request_batch_info:
                                request_batch_info[req_idx] = []
                            
                            retry_batch_info = {
                                'batch_number': f'retry_{retry_count}',  # ë¬¸í•­ì˜ batch_indexì™€ ë™ì¼í•˜ê²Œ
                                'requested_count': shortage,
                                'generated_count': len(retry_questions),
                                'input_tokens': retry_metadata.get('input_tokens', 0),
                                'output_tokens': retry_metadata.get('output_tokens', 0),
                                'total_tokens': retry_metadata.get('total_tokens', 0),
                                'duration_seconds': retry_metadata.get('duration_seconds', 0)
                            }
                            
                            # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                            if 'error' in retry_metadata:
                                retry_batch_info['error'] = retry_metadata['error']
                            
                            request_batch_info[req_idx].append(retry_batch_info)
                            
                            print(f"âœ… ì¬ìš”ì²­ ì™„ë£Œ: {len(retry_questions)}ê°œ ì¶”ê°€ ìƒì„± (ëˆ„ì  {len(questions)}ê°œ)")
                            print(f"ğŸ“Š [ì¬ìš”ì²­ ë°°ì¹˜ ì •ë³´] {retry_batch_info}")
                        else:
                            print(f"âš ï¸ ì¬ìš”ì²­ ì‹¤íŒ¨: ê²°ê³¼ ì—†ìŒ")
                            break
                            
                    except Exception as e:
                        print(f"âŒ ì¬ìš”ì²­ ì—ëŸ¬ ({retry_count}íšŒì°¨): {e}")
                        import traceback
                        traceback.print_exc()
                        break
                
                # ìµœì¢… ê²°ê³¼ í™•ì¸
                if len(questions) < request.generation_count:
                    final_shortage = request.generation_count - len(questions)
                    print(f"âš ï¸ ìµœì¢… ë¶€ì¡±: {final_shortage}ê°œ (ìš”ì²­ {request.generation_count}ê°œ â†’ ìµœì¢… {len(questions)}ê°œ)")
                else:
                    print(f"âœ… ëª©í‘œ ë‹¬ì„±: {len(questions)}ê°œ ìƒì„± ì™„ë£Œ")
                
                # ì´ˆê³¼ ìƒì„±ëœ ê²½ìš° is_used í•„ë“œ ì¶”ê°€ (0, 1 íƒœê¹…)
                if len(questions) > request.generation_count:
                    trimmed = questions[:request.generation_count]
                    excess = questions[request.generation_count:]
                    # ì‚¬ìš©ë¶„ì— is_used=1, ë‚˜ë¨¸ì§€ì— is_used=0 íƒœê·¸ ì¶”ê°€
                    for q in trimmed:
                        q['is_used'] = 1
                    for q in excess:
                        q['is_used'] = 0
                    questions = trimmed + excess
                else:
                    # ìš”ì²­ ìˆ˜ ì´í•˜ë©´ ëª¨ë‘ is_used=1
                    for q in questions:
                        q['is_used'] = 1
                # ìš”ì²­í•œ ë¬¸í•­ ìˆ˜ë§Œ ë°˜í™˜(ë°°ì¹˜ íŒŒì¼ì—ëŠ” is_used=0 í¬í•¨ë˜ì–´ ë’¤ì— ë¶™ìŒ, ë°°ì¹˜ ë°˜í™˜ì—ëŠ” ì‚¬ìš©ëœ ê²ƒë§Œ)
                questions = questions[:request.generation_count]
                
                if questions:
                    # JSON íŒŒì¼ë¡œ ì €ì¥ (ë°°ì¹˜)
                    try:
                        import json
                        from datetime import datetime
                        import os
                        
                        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
                        from app.utils.file_path import parse_school_level_to_path
                        school_level = request.school_level if hasattr(request, 'school_level') else None
                        school_path = parse_school_level_to_path(school_level) if school_level else "default"
                        output_dir = f"storage/{school_path}"
                        os.makedirs(output_dir, exist_ok=True)
                        
                        # íƒ€ì„ìŠ¤íƒ¬í”„
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        
                        # íŒŒì¼ëª… ìƒì„±
                        achievement_code = request.curriculum_info[0].achievement_code if request.curriculum_info and len(request.curriculum_info) > 0 else "unknown"
                        filename = f"questions_batch_{req_idx}_{achievement_code}_{timestamp}.json"
                        filepath = os.path.join(output_dir, filename)
                        
                        # ë°°ì¹˜ë³„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                        batch_info = request_batch_info.get(req_idx, [])
                        
                        # JSON íŒŒì¼ë¡œ ì €ì¥
                        with open(filepath, 'w', encoding='utf-8') as f:
                            json.dump({
                                "metadata": {
                                    "request_index": req_idx,
                                    "achievement_code": achievement_code,
                                    "school_level": school_level,
                                    "total_questions": len(questions),
                                    "requested_count": request.generation_count,
                                    "generated_at": timestamp,
                                    "batches": batch_info
                                },
                                "questions": questions  # ì´ë¯¸ dictë¡œ ë³€í™˜ë¨
                            }, f, ensure_ascii=False, indent=2)
                        
                        print(f"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ (ë°°ì¹˜ {req_idx}): {filepath}")
                        
                    except Exception as e:
                        print(f"âš ï¸ JSON ì €ì¥ ì‹¤íŒ¨ (ë°°ì¹˜ {req_idx}): {e}")
                    
                    # dictë¥¼ Question ê°ì²´ë¡œ ë³€í™˜
                    question_objects = []
                    for q_idx, q_dict in enumerate(questions):
                        try:
                            # passage_infoì˜ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
                            if 'passage_info' in q_dict and isinstance(q_dict['passage_info'], dict):
                                passage_info = q_dict['passage_info']
                                
                                # original_used ì²˜ë¦¬
                                orig_val = passage_info.get('original_used')
                                if orig_val == '' or orig_val is None:
                                    passage_info['original_used'] = True
                                elif isinstance(orig_val, str):
                                    # ë¬¸ìì—´ "true"/"false" ì²˜ë¦¬
                                    passage_info['original_used'] = orig_val.lower() == 'true'
                                
                                # source_type ì²˜ë¦¬
                                src_val = passage_info.get('source_type')
                                if src_val == '' or src_val is None:
                                    passage_info['source_type'] = 'original'
                            
                            question_obj = Question(**q_dict)
                            question_objects.append(question_obj)
                        except Exception as e:
                            print(f"âš ï¸ ë¬¸í•­ ë³€í™˜ ì‹¤íŒ¨ [{q_idx}]: {e}")
                            continue
                    
                    # ë©”íƒ€ë°ì´í„° ìƒì„± (JSON ì €ì¥ê³¼ ë™ì¼í•œ êµ¬ì¡°)
                    batch_info_objects = [BatchInfo(**bi) for bi in batch_info]
                    metadata = GenerationMetadata(
                        request_index=req_idx,
                        achievement_code=achievement_code,
                        school_level=school_level,
                        total_questions=len(question_objects),
                        requested_count=request.generation_count,
                        generated_at=timestamp,
                        batches=batch_info_objects
                    )
                    
                    # dictë¥¼ Question ê°ì²´ë¡œ ë³€í™˜
                    question_objects = []
                    for q_idx, q_dict in enumerate(questions):
                        try:
                            # passage_infoì˜ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
                            if 'passage_info' in q_dict and isinstance(q_dict['passage_info'], dict):
                                passage_info = q_dict['passage_info']
                                
                                # original_used ì²˜ë¦¬
                                orig_val = passage_info.get('original_used')
                                if orig_val == '' or orig_val is None:
                                    passage_info['original_used'] = True
                                elif isinstance(orig_val, str):
                                    # ë¬¸ìì—´ "true"/"false" ì²˜ë¦¬
                                    passage_info['original_used'] = orig_val.lower() == 'true'
                                
                                # source_type ì²˜ë¦¬
                                src_val = passage_info.get('source_type')
                                if src_val == '' or src_val is None:
                                    passage_info['source_type'] = 'original'
                            
                            question_obj = Question(**q_dict)
                            question_objects.append(question_obj)
                        except Exception as e:
                            print(f"âš ï¸ ë¬¸í•­ ë³€í™˜ ì‹¤íŒ¨ [{q_idx}]: {e}")
                            continue
                    
                    responses.append(
                        QuestionGenerationSuccessResponse(
                            success=True,
                            total_questions=len(question_objects),
                            questions=question_objects,
                            metadata=metadata
                        )
                    )
                else:
                    responses.append(
                        QuestionGenerationErrorResponse(
                            success=False,
                            error=ErrorDetail(
                                code="GENERATION_FAILED",
                                message="ë¬¸í•­ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                                details=""
                            )
                        )
                    )
            
            return responses
            
        except Exception as e:
            # ì „ì²´ ì‹¤íŒ¨ ì‹œ ëª¨ë“  ìš”ì²­ì— ëŒ€í•´ ì—ëŸ¬ ì‘ë‹µ ë°˜í™˜
            return [
                QuestionGenerationErrorResponse(
                    success=False,
                    error=ErrorDetail(
                        code="BATCH_ERROR",
                        message="ë°°ì¹˜ ë¬¸í•­ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                        details=str(e)
                    )
                )
                for _ in requests
            ]

