import json
import asyncio
from typing import List, Optional, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions
from app.clients.base import LLMClientBase
from app.clients.api_key_manager import APIKeyManager
from app.schemas.question_generation import Question
from app.core.config import settings


class GeminiClient(LLMClientBase):
    """Google Gemini API í´ë¼ì´ì–¸íŠ¸ - ì—¬ëŸ¬ API í‚¤ ì§€ì› ë° ë³‘ë ¬ ì²˜ë¦¬"""
    
    def __init__(self, api_key: Optional[str] = None, api_keys: Optional[List[str]] = None):
        """
        Args:
            api_key: ë‹¨ì¼ API í‚¤ (í•˜ìœ„ í˜¸í™˜ì„±)
            api_keys: ì—¬ëŸ¬ API í‚¤ ë¦¬ìŠ¤íŠ¸
        """
        if api_keys:
            self.api_key_manager = APIKeyManager(api_keys, strategy=settings.api_key_rotation_strategy)
            self.api_keys = api_keys
        elif api_key:
            self.api_key_manager = APIKeyManager([api_key])
            self.api_keys = [api_key]
        elif settings.gemini_api_key_list:
            self.api_key_manager = APIKeyManager(
                settings.gemini_api_key_list,
                strategy=settings.api_key_rotation_strategy
            )
            self.api_keys = settings.gemini_api_key_list
        else:
            self.api_key_manager = None
            self.api_keys = []
        
        self.current_model = None
        self.current_key = None
        self.max_parallel = min(settings.max_parallel_api_keys, len(self.api_keys) if self.api_keys else 1)
    
    def validate_api_key(self) -> bool:
        """API í‚¤ ìœ íš¨ì„± ê²€ì¦"""
        if not self.api_key_manager or not self.api_keys:
            return False
        return len(self.api_keys) > 0
    
    def _get_model(self, api_key: str, model_name: Optional[str] = None):
        """íŠ¹ì • API í‚¤ë¡œ ëª¨ë¸ ìƒì„±"""
        if model_name is None:
            model_name = settings.gemini_model_name
        genai.configure(api_key=api_key)
        return genai.GenerativeModel(model_name)
    
    async def generate_questions(
        self,
        system_prompt: str,
        user_prompt: str,
        count: int,
        max_retries: int = 3,
        file_paths: Optional[List[str]] = None,
        file_display_names: Optional[List[str]] = None,
        model_name: Optional[str] = None,
        return_metadata: bool = False,
        **kwargs
    ):
        """
        Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸í•­ ìƒì„± (ìë™ ì¬ì‹œë„ ë° í‚¤ ë¡œí…Œì´ì…˜, íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬)
        
        Args:
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            user_prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            count: ìƒì„±í•  ë¬¸í•­ ìˆ˜
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            file_paths: ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            file_display_names: íŒŒì¼ í‘œì‹œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
            return_metadata: Trueì´ë©´ ë©”íƒ€ë°ì´í„° í¬í•¨í•œ Dict ë°˜í™˜, Falseë©´ List[Question] ë°˜í™˜
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            return_metadata=False: List[Question]
            return_metadata=True: Dict[str, Any] with 'questions' and 'metadata'
        """
        if not self.api_key_manager:
            raise ValueError("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # model_nameì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if model_name is None:
            model_name = settings.gemini_model_name
        
        # ë¹ ë¥¸ ì‹¤íŒ¨ ì „í™˜ í™œì„±í™” ì‹œ ì—¬ëŸ¬ í‚¤ë¥¼ ë™ì‹œì— ì‹œë„
        if settings.enable_fast_failover and len(self.api_keys) > 1:
            return await self._generate_with_fast_failover(
                system_prompt, user_prompt, count, 
                file_paths, file_display_names, model_name, **kwargs
            )
        
        # ì¼ë°˜ì ì¸ ìˆœì°¨ ì¬ì‹œë„ ë°©ì‹
        last_error = None
        import time
        start_time = time.time()
        
        for attempt in range(max_retries):
            try:
                api_key = self.api_key_manager.get_next_key()
                if not api_key:
                    raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ Gemini API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                model = self._get_model(api_key, model_name)
                
                # íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì¬ì‹œë„ ì‹œ ë” ì§§ì€ íƒ€ì„ì•„ì›ƒ)
                timeout = settings.api_retry_timeout if attempt > 0 else settings.api_call_timeout
                
                try:
                    response_obj = await asyncio.wait_for(
                        self._call_api_with_files(
                            system_prompt, user_prompt, model, 
                            file_paths, file_display_names, count, model_name,
                            return_response_obj=return_metadata
                        ),
                        timeout=timeout
                    )
                    
                    # return_metadata=Trueë©´ response ê°ì²´, Falseë©´ response.text
                    if return_metadata:
                        response_text = response_obj.text
                    else:
                        response_text = response_obj
                    
                    questions = self._parse_response(response_text, count)
                except asyncio.TimeoutError:
                    # íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ í•´ë‹¹ í‚¤ë¥¼ ì¼ì‹œì ìœ¼ë¡œ ì°¨ë‹¨
                    self.api_key_manager.mark_error(api_key, "timeout")
                    raise Exception(f"API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ ì´ˆê³¼)")
                
                # ì„±ê³µ ì‹œ í˜„ì¬ í‚¤ ì„±ê³µ í‘œì‹œ
                self.api_key_manager.mark_success(api_key)
                
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ìš”ì²­ëœ ê²½ìš°)
                if return_metadata:
                    end_time = time.time()
                    duration = end_time - start_time
                    
                    metadata = {
                        'input_tokens': 0,
                        'output_tokens': 0,
                        'total_tokens': 0,
                        'duration_seconds': round(duration, 2)
                    }
                    
                    # usage_metadata ì¶”ì¶œ
                    print(f"ğŸ” [DEBUG] response ê°ì²´ í™•ì¸: hasattr(usage_metadata) = {hasattr(response_obj, 'usage_metadata')}")
                    if hasattr(response_obj, 'usage_metadata'):
                        usage = response_obj.usage_metadata
                        print(f"ğŸ“Š [í† í° ì •ë³´] usage_metadata: {usage}")
                        metadata['input_tokens'] = getattr(usage, 'prompt_token_count', 0)
                        metadata['output_tokens'] = getattr(usage, 'candidates_token_count', 0)
                        metadata['total_tokens'] = getattr(usage, 'total_token_count', 0)
                        print(f"âœ… [í† í° ì¶”ì¶œ] input={metadata['input_tokens']}, output={metadata['output_tokens']}, total={metadata['total_tokens']}, duration={metadata['duration_seconds']}ì´ˆ")
                    else:
                        print(f"âš ï¸ [WARNING] responseì— usage_metadata ì—†ìŒ. response íƒ€ì…: {type(response_obj)}")
                    
                    return {
                        'questions': questions,
                        'metadata': metadata
                    }
                
                return questions
                
            except asyncio.TimeoutError:
                # íƒ€ì„ì•„ì›ƒ ì—ëŸ¬ëŠ” ì´ë¯¸ ì²˜ë¦¬ë¨
                last_error = Exception("API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ")
                if attempt < max_retries - 1:
                    continue
                else:
                    raise Exception("ëª¨ë“  API í‚¤ì—ì„œ íƒ€ì„ì•„ì›ƒ ë°œìƒ")
            except Exception as e:
                last_error = e
                error_str = str(e).lower()
                
                # íƒ€ì„ì•„ì›ƒ ì—ëŸ¬ ì²˜ë¦¬
                if "timeout" in error_str:
                    if api_key:
                        self.api_key_manager.mark_error(api_key, "timeout")
                # Rate limit ë˜ëŠ” API í‚¤ ì—ëŸ¬ì¸ ê²½ìš°
                elif "rate limit" in error_str or "quota" in error_str or "resource exhausted" in error_str:
                    if api_key:
                        self.api_key_manager.mark_error(api_key, "rate_limit")
                elif "api key" in error_str or "permission" in error_str:
                    if api_key:
                        self.api_key_manager.mark_error(api_key, "invalid_key")
                
                # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ë‹¤ìŒ í‚¤ë¡œ ì¬ì‹œë„
                if attempt < max_retries - 1:
                    continue
                else:
                    raise Exception(f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        raise Exception(f"ëª¨ë“  API í‚¤ ì‹œë„ ì‹¤íŒ¨: {str(last_error)}")
    
    async def _generate_with_fast_failover(
        self,
        system_prompt: str,
        user_prompt: str,
        count: int,
        file_paths: Optional[List[str]] = None,
        file_display_names: Optional[List[str]] = None,
        model_name: Optional[str] = None,
        **kwargs
    ) -> List[Question]:
        """
        ë¹ ë¥¸ ì‹¤íŒ¨ ì „í™˜: ì—¬ëŸ¬ API í‚¤ë¥¼ ë™ì‹œì— ì‹œë„í•˜ê³  ê°€ì¥ ë¹ ë¥¸ ì‘ë‹µ ì‚¬ìš©
        """
        # model_nameì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if model_name is None:
            model_name = settings.gemini_model_name
        
        available_keys = self.api_key_manager._get_available_keys()
        if not available_keys:
            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìµœëŒ€ 3ê°œ í‚¤ë¥¼ ë™ì‹œì— ì‹œë„ (ë„ˆë¬´ ë§ìœ¼ë©´ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„)
        max_concurrent = min(3, len(available_keys))
        keys_to_try = available_keys[:max_concurrent]
        
        # ì—¬ëŸ¬ í‚¤ë¡œ ë™ì‹œì— ì‹œë„
        tasks = []
        for api_key in keys_to_try:
            model = self._get_model(api_key, model_name)
            task = asyncio.create_task(
                self._call_api_with_files(
                    system_prompt, user_prompt, model,
                    file_paths, file_display_names, count, model_name
                )
            )
            tasks.append((task, api_key))
        
        # ê°€ì¥ ë¹ ë¥¸ ì‘ë‹µì„ ê¸°ë‹¤ë¦¼ (race condition)
        done, pending = await asyncio.wait(
            [task for task, _ in tasks],
            return_when=asyncio.FIRST_COMPLETED,
            timeout=settings.api_call_timeout
        )
        
        # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
        for task, api_key in tasks:
            if task in done:
                try:
                    result = await task
                    if result:
                        questions, used_key = result
                        # ì„±ê³µí•œ í‚¤ í‘œì‹œ
                        self.api_key_manager.mark_success(used_key)
                        # ë‚˜ë¨¸ì§€ ì‘ì—… ì·¨ì†Œ
                        for pending_task, _ in tasks:
                            if pending_task not in done:
                                pending_task.cancel()
                        return questions
                except Exception as e:
                    error_str = str(e).lower()
                    if "timeout" in error_str:
                        self.api_key_manager.mark_error(api_key, "timeout")
                    continue
        
        # ëª¨ë“  ì‘ì—…ì´ ì‹¤íŒ¨í•œ ê²½ìš°
        # ë‚˜ë¨¸ì§€ í‚¤ë¡œ ì¬ì‹œë„
        remaining_keys = [k for k in available_keys if k not in keys_to_try]
        if remaining_keys:
            # ì¬ì‹œë„ (ì¼ë°˜ ë°©ì‹, ìµœëŒ€ 3ê°œ)
            for api_key in remaining_keys[:3]:
                try:
                    model = self._get_model(api_key, model_name)
                    response = await asyncio.wait_for(
                        self._call_api_with_files(
                            system_prompt, user_prompt, model,
                            file_paths, file_display_names, count, model_name
                        ),
                        timeout=settings.api_retry_timeout
                    )
                    questions = self._parse_response(response, count)
                    self.api_key_manager.mark_success(api_key)
                    return questions
                except Exception as e:
                    error_str = str(e).lower()
                    if "timeout" in error_str:
                        self.api_key_manager.mark_error(api_key, "timeout")
                    continue
        
        raise Exception("ëª¨ë“  API í‚¤ì—ì„œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    async def generate_questions_batch(
        self,
        system_prompts: List[str],
        user_prompts: List[str],
        counts: List[int],
        file_paths_list: Optional[List[Optional[List[str]]]] = None,
        file_display_names_list: Optional[List[Optional[List[str]]]] = None,
        model_names: Optional[List[str]] = None,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        ë°°ì¹˜ ë¬¸í•­ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬ - ìµœëŒ€ 5ê°œ API í‚¤ ë™ì‹œ ì‚¬ìš©, íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬)
        
        Returns:
            ê° ë°°ì¹˜ì˜ ê²°ê³¼ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸:
            [
                {
                    'questions': List[Question],
                    'metadata': {
                        'input_tokens': int,
                        'output_tokens': int,
                        'total_tokens': int,
                        'duration_seconds': float
                    }
                }
            ]
        """
        if not self.api_key_manager:
            raise ValueError("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì…ë ¥ ê²€ì¦
        if len(system_prompts) != len(user_prompts) or len(system_prompts) != len(counts):
            raise ValueError("system_prompts, user_prompts, countsì˜ ê¸¸ì´ê°€ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.")
        
        # ë°°ì¹˜ í¬ê¸° ê²°ì • (ìµœëŒ€ 5ê°œ ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ ìˆ˜)
        batch_size = min(self.max_parallel, len(system_prompts), len(self.api_keys))
        
        all_results = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(system_prompts), batch_size):
            batch_sys_prompts = system_prompts[i:i+batch_size]
            batch_usr_prompts = user_prompts[i:i+batch_size]
            batch_counts = counts[i:i+batch_size]
            batch_file_paths_list = file_paths_list[i:i+batch_size] if file_paths_list else [None] * len(batch_sys_prompts)
            batch_file_display_names_list = file_display_names_list[i:i+batch_size] if file_display_names_list else [None] * len(batch_sys_prompts)
            batch_model_names = model_names[i:i+batch_size] if model_names else [settings.gemini_model_name] * len(batch_sys_prompts)
            
            # ì´ë²ˆ ë°°ì¹˜ì— ì‚¬ìš©í•  API í‚¤ ê°€ì ¸ì˜¤ê¸°
            batch_api_keys = self.api_key_manager.get_keys_for_batch(len(batch_sys_prompts))
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬ (íƒ€ì„ì•„ì›ƒ í¬í•¨)
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor(max_workers=len(batch_api_keys)) as executor:
                futures = []
                
                for j, (sys_prompt, usr_prompt, count) in enumerate(zip(batch_sys_prompts, batch_usr_prompts, batch_counts)):
                    api_key = batch_api_keys[j % len(batch_api_keys)]
                    file_paths = batch_file_paths_list[j] if j < len(batch_file_paths_list) else None
                    file_display_names = batch_file_display_names_list[j] if j < len(batch_file_display_names_list) else None
                    model_name = batch_model_names[j] if j < len(batch_model_names) else settings.gemini_model_name
                    
                    future = loop.run_in_executor(
                        executor,
                        self._generate_single_question_sync,
                        api_key,
                        sys_prompt,
                        usr_prompt,
                        count,
                        file_paths,
                        file_display_names,
                        model_name,
                        **kwargs
                    )
                    futures.append(future)
                
                # íƒ€ì„ì•„ì›ƒì„ í¬í•¨í•œ ê²°ê³¼ ìˆ˜ì§‘
                batch_results = []
                for future in futures:
                    try:
                        # ê° ì‘ì—…ì— ê°œë³„ íƒ€ì„ì•„ì›ƒ ì„¤ì •
                        result = await asyncio.wait_for(
                            asyncio.wrap_future(future),
                            timeout=settings.api_call_timeout
                        )
                        batch_results.append(result)
                    except asyncio.TimeoutError:
                        # íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜
                        print(f"âš ï¸ API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ ({settings.api_call_timeout}ì´ˆ ì´ˆê³¼)")
                        batch_results.append({
                            'questions': [],
                            'metadata': {
                                'input_tokens': 0,
                                'output_tokens': 0,
                                'total_tokens': 0,
                                'duration_seconds': settings.api_call_timeout,
                                'error': 'Timeout'
                            }
                        })
                    except Exception as e:
                        print(f"âš ï¸ API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)[:100]}")
                        batch_results.append({
                            'questions': [],
                            'metadata': {
                                'input_tokens': 0,
                                'output_tokens': 0,
                                'total_tokens': 0,
                                'duration_seconds': 0,
                                'error': str(e)[:100]
                            }
                        })
                
                # ê²°ê³¼ ì¶”ê°€
                all_results.extend(batch_results)
        
        return all_results
    
    def _generate_single_question_sync(
        self,
        api_key: str,
        system_prompt: str,
        user_prompt: str,
        count: int,
        file_paths: Optional[List[str]] = None,
        file_display_names: Optional[List[str]] = None,
        model_name: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë‹¨ì¼ ë¬¸í•­ ìƒì„± (ThreadPoolExecutorìš©)
        ì£¼ì˜: íƒ€ì„ì•„ì›ƒì€ ìƒìœ„ ë ˆë²¨ì˜ asyncio.wait_forì—ì„œ ì²˜ë¦¬ë¨
        
        Returns:
            {
                'questions': List[Question],
                'metadata': {
                    'input_tokens': int,
                    'output_tokens': int,
                    'total_tokens': int,
                    'duration_seconds': float
                }
            }
        """
        # model_nameì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if model_name is None:
            model_name = settings.gemini_model_name
        import time
        start_time = time.time()
        
        try:
            model = self._get_model(api_key, model_name)
            
            # ë™ê¸° ë°©ì‹ìœ¼ë¡œ íŒŒì¼ ì—…ë¡œë“œ ë° API í˜¸ì¶œ
            import os
            from app.schemas.question_generation import MultipleQuestion
            
            # JSON ìŠ¤í‚¤ë§ˆ ìƒì„±
            question_schema_raw = MultipleQuestion.model_json_schema()
            question_schema = self._convert_schema_for_google_genai(question_schema_raw)
            
            # íŒŒì¼ ì—…ë¡œë“œ
            uploaded_files = []
            if file_paths:
                for idx, path in enumerate(file_paths):
                    if path and os.path.exists(path):
                        try:
                            display_name = None
                            if file_display_names and idx < len(file_display_names):
                                display_name = file_display_names[idx]
                            if not display_name:
                                display_name = os.path.basename(path)
                            
                            # upload_fileì€ pathë¥¼ ì²« ë²ˆì§¸ ìœ„ì¹˜ ì¸ìë¡œ ë°›ìŒ
                            uploaded_file = genai.upload_file(
                                path,
                                display_name=display_name if display_name else None
                            )
                            uploaded_files.append(uploaded_file)
                        except Exception as e:
                            print(f"âš ï¸ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {path} - {e}")
            
            # êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„¤ì •
            generation_config = genai.GenerationConfig(
                temperature=kwargs.get("temperature", 0.7),
                top_p=kwargs.get("top_p", 0.95),
                top_k=kwargs.get("top_k", 40),
                response_mime_type="application/json",
                response_schema=question_schema
            )
            
            structured_model = genai.GenerativeModel(
                model_name=model_name,
                generation_config=generation_config
            )
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # API í˜¸ì¶œ
            if uploaded_files:
                content_list = uploaded_files + [full_prompt]
                response = structured_model.generate_content(content_list)
            else:
                response = structured_model.generate_content(full_prompt)
            
            # ì†Œìš” ì‹œê°„ ê³„ì‚°
            end_time = time.time()
            duration = end_time - start_time
            
            # í† í° ì •ë³´ ì¶”ì¶œ
            metadata = {
                'input_tokens': 0,
                'output_tokens': 0,
                'total_tokens': 0,
                'duration_seconds': round(duration, 2)
            }
            
            # Gemini APIì˜ usage_metadataì—ì„œ í† í° ì •ë³´ ì¶”ì¶œ
            print(f"ğŸ” [DEBUG] response ê°ì²´ í™•ì¸: hasattr(usage_metadata) = {hasattr(response, 'usage_metadata')}")
            if hasattr(response, 'usage_metadata'):
                usage = response.usage_metadata
                print(f"ğŸ“Š [í† í° ì •ë³´] usage_metadata: {usage}")
                metadata['input_tokens'] = getattr(usage, 'prompt_token_count', 0)
                metadata['output_tokens'] = getattr(usage, 'candidates_token_count', 0)
                metadata['total_tokens'] = getattr(usage, 'total_token_count', 0)
                print(f"âœ… [í† í° ì¶”ì¶œ] input={metadata['input_tokens']}, output={metadata['output_tokens']}, total={metadata['total_tokens']}, duration={metadata['duration_seconds']}ì´ˆ")
            else:
                print(f"âš ï¸ [WARNING] responseì— usage_metadata ì—†ìŒ. response íƒ€ì…: {type(response)}")
                print(f"âš ï¸ [WARNING] response ì†ì„±: {dir(response)}")
            
            questions = self._parse_response(response.text, count)
            
            # ì„±ê³µ í‘œì‹œ
            self.api_key_manager.mark_success(api_key)
            
            return {
                'questions': questions,
                'metadata': metadata
            }
            
        except google_exceptions.ResourceExhausted as e:
            # Rate limit ì—ëŸ¬
            self.api_key_manager.mark_error(api_key, "rate_limit")
            end_time = time.time()
            return {
                'questions': [],
                'metadata': {
                    'input_tokens': 0,
                    'output_tokens': 0,
                    'total_tokens': 0,
                    'duration_seconds': round(end_time - start_time, 2),
                    'error': f"Rate limit exceeded: {str(e)}"
                }
            }
        except Exception as e:
            error_str = str(e).lower()
            if "timeout" in error_str or "timed out" in error_str:
                self.api_key_manager.mark_error(api_key, "timeout")
            elif "api key" in error_str or "permission" in error_str:
                self.api_key_manager.mark_error(api_key, "invalid_key")
            end_time = time.time()
            return {
                'questions': [],
                'metadata': {
                    'input_tokens': 0,
                    'output_tokens': 0,
                    'total_tokens': 0,
                    'duration_seconds': round(end_time - start_time, 2),
                    'error': str(e)
                }
            }
    
    async def _call_api(self, prompt: str, model) -> str:
        """Gemini API í˜¸ì¶œ (ë¹„ë™ê¸° ë˜í¼)"""
        # ë™ê¸° APIë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: model.generate_content(prompt)
        )
        
        return response.text
    
    async def _call_api_with_files(
        self,
        system_prompt: str,
        user_prompt: str,
        model,
        file_paths: Optional[List[str]] = None,
        file_display_names: Optional[List[str]] = None,
        count: int = 10,
        model_name: Optional[str] = None,
        return_response_obj: bool = False,
        **kwargs
    ):
        """íŒŒì¼ì´ í¬í•¨ëœ Gemini API í˜¸ì¶œ (êµ¬ì¡°í™”ëœ ì¶œë ¥)
        
        Args:
            return_response_obj: Trueë©´ response ê°ì²´ ì „ì²´ ë°˜í™˜, Falseë©´ response.textë§Œ ë°˜í™˜
            
        Returns:
            response ê°ì²´ ë˜ëŠ” response.text
        """
        # model_nameì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if model_name is None:
            model_name = settings.gemini_model_name
        
        import os
        from app.schemas.question_generation import MultipleQuestion
        
        # JSON ìŠ¤í‚¤ë§ˆ ìƒì„±
        question_schema_raw = MultipleQuestion.model_json_schema()
        question_schema = self._convert_schema_for_google_genai(question_schema_raw)
        
        # íŒŒì¼ ì—…ë¡œë“œ
        uploaded_files = []
        if file_paths:
            for idx, path in enumerate(file_paths):
                if path and os.path.exists(path):
                    try:
                        display_name = None
                        if file_display_names and idx < len(file_display_names):
                            display_name = file_display_names[idx]
                        if not display_name:
                            display_name = os.path.basename(path)
                        
                        # upload_fileì€ pathë¥¼ ì²« ë²ˆì§¸ ìœ„ì¹˜ ì¸ìë¡œ ë°›ìŒ
                        uploaded_file = genai.upload_file(
                            path,
                            display_name=display_name if display_name else None
                        )
                        uploaded_files.append(uploaded_file)
                    except Exception as e:
                        print(f"âš ï¸ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {path} - {e}")
        
        # êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„¤ì •
        temperature = kwargs.get("temperature", 0.7) if kwargs else 0.7
        top_p = kwargs.get("top_p", 0.95) if kwargs else 0.95
        top_k = kwargs.get("top_k", 40) if kwargs else 40
        
        generation_config = genai.GenerationConfig(
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            response_mime_type="application/json",
            response_schema=question_schema
        )
        
        # ëª¨ë¸ ì¬ìƒì„± (êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„¤ì • í¬í•¨)
        structured_model = genai.GenerativeModel(
            model_name=model_name,
            generation_config=generation_config
        )
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        # API í˜¸ì¶œ
        loop = asyncio.get_event_loop()
        if uploaded_files:
            content_list = uploaded_files + [full_prompt]
            response = await loop.run_in_executor(
                None,
                lambda: structured_model.generate_content(content_list)
            )
        else:
            response = await loop.run_in_executor(
                None,
                lambda: structured_model.generate_content(full_prompt)
            )
        
        # return_response_objì— ë”°ë¼ ë°˜í™˜ í˜•ì‹ ê²°ì •
        if return_response_obj:
            return response  # ì „ì²´ response ê°ì²´ ë°˜í™˜
        else:
            return response.text  # textë§Œ ë°˜í™˜
    
    def _parse_response(self, response_text: str, expected_count: int) -> List[Question]:
        """
        API ì‘ë‹µì„ Question ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±
        MultipleQuestion í˜•ì‹ìœ¼ë¡œ ì‘ë‹µì´ ì˜¤ëŠ” ê²½ìš° ì²˜ë¦¬
        """
        from app.schemas.question_generation import MultipleQuestion, LLMQuestion
        
        questions = []
        
        try:
            # JSON íŒŒì‹±
            data = json.loads(response_text)
            
            # MultipleQuestion í˜•ì‹ì¸ ê²½ìš°
            if "questions" in data:
                multiple_question = MultipleQuestion(**data)
                for idx, llm_question in enumerate(multiple_question.questions[:expected_count], 1):
                    question = self._convert_llm_question_to_question(llm_question, idx)
                    if question:
                        questions.append(question)
            # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì¸ ê²½ìš°
            elif isinstance(data, list):
                for idx, item in enumerate(data[:expected_count], 1):
                    if isinstance(item, dict) and "question_text" in item:
                        # LLMQuestion í˜•ì‹
                        llm_question = LLMQuestion(**item)
                        question = self._convert_llm_question_to_question(llm_question, idx)
                    else:
                        # ê¸°ì¡´ í˜•ì‹
                        question = self._parse_question(item, idx)
                    if question:
                        questions.append(question)
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ì‘ë‹µ í…ìŠ¤íŠ¸: {response_text[:500]}")
        except Exception as e:
            print(f"âš ï¸ ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        
        return questions
    
    def _convert_llm_question_to_question(
        self, 
        llm_question, 
        question_number: int
    ) -> Optional[Question]:
        """LLMQuestionì„ Question í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            from app.schemas.question_generation import (
                Question, PassageInfo, QuestionText, LLMQuestion
            )
            
            # dictì¸ ê²½ìš° LLMQuestionìœ¼ë¡œ ë³€í™˜
            if isinstance(llm_question, dict):
                llm_question = LLMQuestion(**llm_question)
            elif not isinstance(llm_question, LLMQuestion):
                return None
            
            # passage ì‚¬ìš© ì—¬ë¶€ íŒë‹¨ (ë¹ˆ ë¬¸ìì—´ì´ë‚˜ None ì²˜ë¦¬)
            passage = llm_question.passage
            
            # passageê°€ Noneì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° ì²˜ë¦¬
            if not passage or (isinstance(passage, str) and not passage.strip()):
                has_passage = False
                original_used = False  # ëª…ì‹œì ìœ¼ë¡œ False í• ë‹¹
                source_type = "none"
            elif passage == "1":
                has_passage = False
                original_used = True  # ëª…ì‹œì ìœ¼ë¡œ True í• ë‹¹
                source_type = "original"
            else:
                has_passage = True
                original_used = True  # ëª…ì‹œì ìœ¼ë¡œ True í• ë‹¹
                source_type = "modified"
            
            return Question(
                question_id=str(question_number),
                question_number=question_number,
                passage_info=PassageInfo(
                    original_used=original_used,
                    source_type=source_type
                ),
                question_text=QuestionText(
                    text=llm_question.question_text,
                    modified_passage=passage if has_passage else None,
                    box_content=llm_question.reference_text if llm_question.reference_text and llm_question.reference_text.strip() else None
                ),
                choices=llm_question.choices,
                correct_answer=llm_question.correct_answer,
                explanation=llm_question.explanation,
                llm_difficulty=llm_question.llm_difficulty
            )
        except Exception as e:
            print(f"âš ï¸ ë¬¸í•­ ë³€í™˜ ì‹¤íŒ¨ [Q{question_number}]: {e}")
            return None
    
    def _convert_schema_for_google_genai(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """
        Pydantic JSON ìŠ¤í‚¤ë§ˆë¥¼ Google Generative AI í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë‹¨ìˆœí™” ë²„ì „)
        - $refë¥¼ ì¸ë¼ì¸ìœ¼ë¡œ í•´ê²°
        - title, default, examples ë“± ì§€ì›í•˜ì§€ ì•ŠëŠ” í•„ë“œ ì œê±°
        - anyOfëŠ” ì²« ë²ˆì§¸ non-null ì˜µì…˜ ì„ íƒ
        """
        def resolve_ref(ref_path: str, defs: Dict) -> Dict:
            """$ref ê²½ë¡œë¥¼ í•´ê²°í•´ ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ë°˜í™˜"""
            if ref_path.startswith("#/$defs/"):
                def_name = ref_path.split("/")[-1]
                return defs.get(def_name, {})
            return {}
        
        def clean_schema_recursive(obj: Any, defs: Dict) -> Any:
            """ì¬ê·€ì ìœ¼ë¡œ ìŠ¤í‚¤ë§ˆ ì •ë¦¬"""
            if isinstance(obj, dict):
                # $refê°€ ìˆìœ¼ë©´ í•´ê²°
                if "$ref" in obj:
                    resolved = resolve_ref(obj["$ref"], defs)
                    return clean_schema_recursive(resolved, defs)
                
                result = {}
                for key, value in obj.items():
                    # ì§€ì›í•˜ì§€ ì•ŠëŠ” í•„ë“œ ì œê±°
                    if key in ["title", "default", "examples", "example", "$defs", 
                              "additionalProperties", "const", "format", "$schema", "$id",
                              "minimum", "maximum", "minLength", "maxLength", "pattern",
                              "minItems", "maxItems", "uniqueItems", "multipleOf"]:
                        continue
                    
                    # anyOf ì²˜ë¦¬: nullì´ ì•„ë‹Œ ì²« ë²ˆì§¸ ì˜µì…˜ ì„ íƒ
                    if key == "anyOf":
                        if isinstance(value, list) and len(value) > 0:
                            for option in value:
                                if isinstance(option, dict):
                                    if "$ref" in option:
                                        resolved = resolve_ref(option["$ref"], defs)
                                        cleaned = clean_schema_recursive(resolved, defs)
                                        if cleaned.get("type") != "null":
                                            result.update(cleaned)
                                            break
                                    elif option.get("type") != "null":
                                        result.update(clean_schema_recursive(option, defs))
                                        break
                        continue
                    
                    # oneOf, allOfë„ ë¹„ìŠ·í•˜ê²Œ ì²˜ë¦¬
                    if key == "oneOf":
                        if isinstance(value, list) and len(value) > 0:
                            result.update(clean_schema_recursive(value[0], defs))
                        continue
                    
                    if key == "allOf":
                        if isinstance(value, list):
                            for item in value:
                                result.update(clean_schema_recursive(item, defs))
                        continue
                    
                    # ì¬ê·€ì ìœ¼ë¡œ ì •ë¦¬
                    result[key] = clean_schema_recursive(value, defs)
                
                return result
            elif isinstance(obj, list):
                return [clean_schema_recursive(item, defs) for item in obj]
            else:
                return obj
        
        # ìŠ¤í‚¤ë§ˆ ë³µì‚¬ ë° $defs ì¶”ì¶œ
        schema_copy = schema.copy()
        defs = schema_copy.pop("$defs", {})
        
        # ì •ë¦¬ëœ ìŠ¤í‚¤ë§ˆ ìƒì„±
        cleaned = clean_schema_recursive(schema_copy, defs)
        
        # required ê²€ì¦: propertiesì— ì—†ëŠ” í•„ë“œ ì œê±°
        if isinstance(cleaned, dict) and "required" in cleaned and "properties" in cleaned:
            properties = cleaned.get("properties", {})
            required = cleaned.get("required", [])
            if isinstance(properties, dict) and isinstance(required, list):
                valid_required = [f for f in required if f in properties]
                if valid_required:
                    cleaned["required"] = valid_required
                else:
                    cleaned.pop("required", None)
        
        return cleaned if cleaned else {"type": "object"}
    
    def _parse_question(self, data: dict, question_number: int) -> Optional[Question]:
        """ë‹¨ì¼ ë¬¸í•­ íŒŒì‹±"""
        try:
            from app.schemas.question_generation import (
                Question, PassageInfo, QuestionText, Choice
            )
            
            # original_used ê°’ ì²˜ë¦¬ (ë¹ˆ ë¬¸ìì—´ì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            passage_info_data = data.get("passage_info", {})
            original_used_value = passage_info_data.get("original_used", True)
            
            if original_used_value == "" or original_used_value is None:
                original_used_value = True
            
            source_type_value = passage_info_data.get("source_type") or "original"
            
            return Question(
                question_id=str(data.get("question_id", question_number)),
                question_number=question_number,
                passage_info=PassageInfo(
                    original_used=original_used_value,
                    source_type=source_type_value
                ),
                question_text=QuestionText(
                    text=data.get("question_text", {}).get("text", ""),
                    modified_passage=data.get("question_text", {}).get("modified_passage"),
                    box_content=data.get("question_text", {}).get("box_content")
                ),
                choices=[
                    Choice(number=choice["number"], text=choice["text"])
                    for choice in data.get("choices", [])
                ],
                correct_answer=str(data.get("correct_answer", "")),
                explanation=data.get("explanation", "")
            )
        except Exception as e:
            print(f"âš ï¸ _parse_question ì—ëŸ¬: {e}")
            import traceback
            traceback.print_exc()
            return None

